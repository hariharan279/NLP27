{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c786c7d-d4f8-4191-8198-2c06863a82c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 1434/1434 [03:17<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss: 0.25933822898539705, Train Accuracy: 0.04079497907949791\n",
      "Validation Loss: 0.2141675880551338, Validation Accuracy: 0.182\n",
      "Validation Hierarchical F1 Score: 0.9223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 1434/1434 [03:22<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Train Loss: 0.21372416453901028, Train Accuracy: 0.15271966527196654\n",
      "Validation Loss: 0.20962062710523605, Validation Accuracy: 0.18\n",
      "Validation Hierarchical F1 Score: 0.9199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 1434/1434 [03:23<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Train Loss: 0.1711783012014137, Train Accuracy: 0.2853905160390516\n",
      "Validation Loss: 0.21437218767404556, Validation Accuracy: 0.19\n",
      "Validation Hierarchical F1 Score: 0.9211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 1434/1434 [03:22<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Train Loss: 0.1300839146916109, Train Accuracy: 0.42102510460251047\n",
      "Validation Loss: 0.2347697988152504, Validation Accuracy: 0.162\n",
      "Validation Hierarchical F1 Score: 0.9166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 1434/1434 [03:22<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Train Loss: 0.09587716946126361, Train Accuracy: 0.5529986052998606\n",
      "Validation Loss: 0.254372066617012, Validation Accuracy: 0.154\n",
      "Validation Hierarchical F1 Score: 0.9163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 1434/1434 [03:22<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Train Loss: 0.07299370845718728, Train Accuracy: 0.6391213389121339\n",
      "Validation Loss: 0.27589485675096515, Validation Accuracy: 0.162\n",
      "Validation Hierarchical F1 Score: 0.915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 1434/1434 [03:22<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Train Loss: 0.05661660767351297, Train Accuracy: 0.7248953974895398\n",
      "Validation Loss: 0.2986681606769562, Validation Accuracy: 0.154\n",
      "Validation Hierarchical F1 Score: 0.9144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 1434/1434 [03:23<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15, Train Loss: 0.04596575029305412, Train Accuracy: 0.7681311018131102\n",
      "Validation Loss: 0.3104943144619465, Validation Accuracy: 0.156\n",
      "Validation Hierarchical F1 Score: 0.9169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 1434/1434 [03:22<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15, Train Loss: 0.038218288326335906, Train Accuracy: 0.8014295676429568\n",
      "Validation Loss: 0.3443458403348923, Validation Accuracy: 0.152\n",
      "Validation Hierarchical F1 Score: 0.9068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 1434/1434 [03:22<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15, Train Loss: 0.033502350860314944, Train Accuracy: 0.822350069735007\n",
      "Validation Loss: 0.36244809663295746, Validation Accuracy: 0.14\n",
      "Validation Hierarchical F1 Score: 0.9101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 1434/1434 [03:23<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15, Train Loss: 0.030519718786072722, Train Accuracy: 0.8355997210599722\n",
      "Validation Loss: 0.3575068064332008, Validation Accuracy: 0.154\n",
      "Validation Hierarchical F1 Score: 0.9138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 1434/1434 [03:22<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15, Train Loss: 0.027118770998564575, Train Accuracy: 0.8537308228730823\n",
      "Validation Loss: 0.38279775312542913, Validation Accuracy: 0.156\n",
      "Validation Hierarchical F1 Score: 0.9102000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 1434/1434 [03:22<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15, Train Loss: 0.02451835622606056, Train Accuracy: 0.8683751743375174\n",
      "Validation Loss: 0.3966154611110687, Validation Accuracy: 0.152\n",
      "Validation Hierarchical F1 Score: 0.9159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 1434/1434 [03:22<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15, Train Loss: 0.023795079214475993, Train Accuracy: 0.8673291492329149\n",
      "Validation Loss: 0.3985769712328911, Validation Accuracy: 0.148\n",
      "Validation Hierarchical F1 Score: 0.9122000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 1434/1434 [03:22<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15, Train Loss: 0.0217208201141159, Train Accuracy: 0.8847629009762901\n",
      "Validation Loss: 0.3931615781188011, Validation Accuracy: 0.158\n",
      "Validation Hierarchical F1 Score: 0.9136\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Define label mapping\n",
    "label_mapping = {\n",
    "    'Appeal to authority': 0, 'Appeal to fear/prejudice': 1, 'Bandwagon': 2,\n",
    "    'Black-and-white Fallacy/Dictatorship': 3, 'Causal Oversimplification': 4,\n",
    "    'Doubt': 5, 'Exaggeration/Minimisation': 6, 'Flag-waving': 7, \n",
    "    'Glittering generalities (Virtue)': 8, 'Loaded Language': 9, \n",
    "    \"Misrepresentation of Someone's Position (Straw Man)\": 10, \n",
    "    'Name calling/Labeling': 11, 'Obfuscation, Intentional vagueness, Confusion': 12, \n",
    "    'Presenting Irrelevant Data (Red Herring)': 13, 'Reductio ad hitlerum': 14, 'Repetition': 15, \n",
    "    'Slogans': 16, 'Smears': 17, 'Thought-terminating cliché': 18, 'Whataboutism': 19\n",
    "}\n",
    "\n",
    "# Define the MemeDataset class\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, annotation_file, tokenizer, label_mapping):\n",
    "        with open(annotation_file, 'r', encoding='utf-8') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['text']\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "\n",
    "        label_indices = [self.label_mapping[label] for label in item['labels']]\n",
    "        labels = torch.zeros(len(self.label_mapping))\n",
    "        labels[label_indices] = 1\n",
    "\n",
    "        return input_ids, attention_mask, labels\n",
    "\n",
    "# Define the MemeClassifier model\n",
    "class MemeClassifier(nn.Module):\n",
    "    def __init__(self, text_model, num_labels):\n",
    "        super(MemeClassifier, self).__init__()\n",
    "        self.text_model = text_model\n",
    "        self.classifier = nn.Linear(self.text_model.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = outputs.pooler_output\n",
    "        logits = self.classifier(text_features)\n",
    "        return logits\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = MemeClassifier(BertModel.from_pretrained('bert-base-uncased'), num_labels=len(label_mapping))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training and validation datasets and dataloaders\n",
    "train_dataset = MemeDataset(\n",
    "    annotation_file='C:\\\\Users\\\\harih\\\\Downloads\\\\27\\\\semeval2024_dev_release\\\\semeval2024_dev_release\\\\subtask1\\\\train_filtered.json',\n",
    "    tokenizer=tokenizer,\n",
    "    label_mapping=label_mapping\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "val_dataset = MemeDataset(\n",
    "    annotation_file='C:\\\\Users\\\\harih\\\\Downloads\\\\27\\\\semeval2024_dev_release\\\\semeval2024_dev_release\\\\subtask1\\\\validation.json',\n",
    "    tokenizer=tokenizer,\n",
    "    label_mapping=label_mapping\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Training loop with validation\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, train_correct, train_total = 0, 0, 0\n",
    "    for input_ids, attention_mask, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        with torch.no_grad():\n",
    "            predictions = torch.sigmoid(logits).round()\n",
    "            train_correct += (predictions == labels).all(dim=1).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = train_correct / train_total\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss}, Train Accuracy: {train_accuracy}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in val_loader:\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            predictions = torch.sigmoid(logits).round()\n",
    "            val_correct += (predictions == labels).all(dim=1).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_preds.append(predictions.cpu())\n",
    "            \n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "    print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "    # Calculate hierarchical F1 score\n",
    "    all_labels_np = torch.cat(all_labels, dim=0).numpy()\n",
    "    all_preds_np = torch.cat(all_preds, dim=0).numpy()\n",
    "    h_f1 = hierarchical_f1_score(all_labels_np, all_preds_np, label_mapping, hierarchy)\n",
    "    print(f\"Validation Hierarchical F1 Score: {h_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e96218a-3c24-446c-a969-58e7ccde3451",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m all_labels, all_preds \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m input_ids, attention_mask, labels \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m      6\u001b[0m         input_ids, attention_mask, labels \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device), attention_mask\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m         logits \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_loader' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "val_loss, val_correct, val_total = 0, 0, 0\n",
    "all_labels, all_preds = [], []\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, labels in val_loader:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        val_loss += loss.item()\n",
    "        predictions = torch.sigmoid(logits).round()\n",
    "        val_correct += (predictions == labels).all(dim=1).sum().item()\n",
    "        val_total += labels.size(0)\n",
    "        all_labels.append(labels.cpu())\n",
    "        all_preds.append(predictions.cpu())\n",
    "            \n",
    "\n",
    "val_loss = val_loss / len(val_loader)\n",
    "val_accuracy = val_correct / val_total\n",
    "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "    # Calculate hierarchical F1 score\n",
    "all_labels_np = torch.cat(all_labels, dim=0).numpy()\n",
    "all_preds_np = torch.cat(all_preds, dim=0).numpy()\n",
    "h_f1 = hierarchical_f1_score(all_labels_np, all_preds_np, label_mapping, hierarchy)\n",
    "print(f\"Validation Hierarchical F1 Score: {h_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8ec2629-77a8-4c60-90ec-3504c00aee5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hierarchy = {\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a95c0a2c-45e4-47d7-a417-0c0924b8a3ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "persuasion_hierarchy = {\n",
    "    'Persuasion': ['Ethos', 'Pathos', 'Logos'],\n",
    "    'Ethos': ['Ad Hominem', 'Bandwagon', 'Appeal to Authority', 'Glittering Generalities', 'Transfer'],\n",
    "    'Pathos': ['Appeal to Emotion', 'Exaggeration', 'Loaded Language', 'Flag Waving', 'Appeal to Fear', 'Transfer'],\n",
    "    'Logos': ['Justification', 'Reasoning', 'Repetition', 'Intentional Vagueness'],\n",
    "    'Ad Hominem': ['Name Calling', 'Doubt', 'Smears', 'Reductio ad Hitlerum', 'Whataboutism'],\n",
    "    'Justification': ['Bandwagon', 'Appeal to Authority', 'Flag Waving', 'Appeal to Fear', 'Slogans'],\n",
    "    'Reasoning': ['Distraction', 'Simplification'],\n",
    "    'Distraction': ['Straw Man', 'Red Herring', 'Whataboutism'],\n",
    "    'Simplification': ['Causal Oversimplification', 'Black & White Fallacy', 'Thought Terminating Cliche'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6e12c82-1b62-4cb7-86c2-27a6ab8cba29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to get all ancestors of a technique in the hierarchy\n",
    "def get_ancestors(technique, hierarchy):\n",
    "    ancestors = []\n",
    "    for parent, children in hierarchy.items():\n",
    "        if technique in children:\n",
    "            ancestors.append(parent)\n",
    "            ancestors.extend(get_ancestors(parent, hierarchy))\n",
    "    return ancestors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d58fc12f-71d4-4e87-839e-96892e10ab0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Assuming your labels are in string format as used in the hierarchy\n",
    "def hierarchical_f1_score(y_true, y_pred, label_mapping, hierarchy):\n",
    "    \"\"\"Calculate the hierarchical F1 score considering the class hierarchy.\n",
    "\n",
    "    Args:\n",
    "        y_true (list of list of strings): True labels.\n",
    "        y_pred (list of list of strings): Predicted labels.\n",
    "        label_mapping (dict): Mapping from label names to indices.\n",
    "        hierarchy (dict): Hierarchy of labels.\n",
    "\n",
    "    Returns:\n",
    "        float: The hierarchical F1 score.\n",
    "    \"\"\"\n",
    "    # Reverse the label mapping to convert indices to names\n",
    "    index_to_label = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "    # Flatten the true and predicted labels to include their ancestors\n",
    "    y_true_flat = []\n",
    "    y_pred_flat = []\n",
    "    for label_set in y_true:\n",
    "        for label in label_set:\n",
    "            label_name = index_to_label[label]\n",
    "            y_true_flat.extend(get_ancestors(label_name, hierarchy) + [label_name])\n",
    "    for label_set in y_pred:\n",
    "        for label in label_set:\n",
    "            label_name = index_to_label[label]\n",
    "            y_pred_flat.extend(get_ancestors(label_name, hierarchy) + [label_name])\n",
    "\n",
    "    # Convert labels names to indices for F1 calculation\n",
    "    y_true_indices = [label_mapping[label] for label in y_true_flat]\n",
    "    y_pred_indices = [label_mapping[label] for label in y_pred_flat]\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_true_indices, y_pred_indices, average='micro')\n",
    "    return f1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d2c2d6f-52c7-4251-9d5c-bae140c053c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f62f5a5-bec9-4b20-b012-81c086323022",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to C:\\Users\\harih\\Downloads\\27\\bert_meme_model_subtask1.pth\n"
     ]
    }
   ],
   "source": [
    "model_path = 'C:\\\\Users\\\\harih\\\\Downloads\\\\27\\\\bert_meme_model_subtask1.pth'\n",
    "save_model(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0c3e3a-3d9e-4929-994e-ad5e123ca91b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
